{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to overfitting\n",
    "\n",
    "In the previous lab we have designed an MLP network for image classification, and we have trained it in a rather straightforward fashion, which in practice is a bit risky. Indeed, our model was designed and trained for minimizing the loss on the training set for a given number of epochs. As such, we might end up with a model that is very good at predicting the data from this training set, but is unable to _generalize_ to unseen/new data (e.g., from the test set). This behavior is called **overfitting** and is a major problem in deep learning, wich we adress in this script.\n",
    "\n",
    "<center><a href=\"https://medium.com/geekculture/investigating-underfitting-and-overfitting-70382835e45c\">\n",
    "    <img src=\"https://miro.medium.com/max/1400/1*OeJVQ7sEvOJGxfGSCpbzyA.png\" width=\"600\"></a></center>\n",
    "    \n",
    "More specifically, we study two common sources of overfitting:\n",
    "\n",
    "- Training for too many epochs.\n",
    "- Using an over-parametrized (i.e., too large) model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and model\n",
    "\n",
    "As in the previous lab, we work with the MNIST dataset and we use a simple 3-layer MLP. We provide the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data repository\n",
    "data_dir = \"../data/\"\n",
    "\n",
    "# Load the MNIST training dataset (in this script we don't need the test set), and take a small subset\n",
    "data_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    data_dir, train=True, download=True, transform=data_transforms\n",
    ")\n",
    "num_classes = len(train_data.classes)\n",
    "train_data = Subset(train_data, torch.arange(2000).tolist())\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP classifier\n",
    "class MLPClassif(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLPClassif, self).__init__()\n",
    "        self.input_layer = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU())\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size), nn.ReLU()\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.input_layer(x)\n",
    "        y = self.hidden_layer(y)\n",
    "        out = self.output_layer(y)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Initialization function\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The validation set\n",
    "\n",
    "Until now, we have splitted our data into two sets: training and testing. A key ingredient in avoiding overfitting is the usage of an additional _validation_ (or _development_) set. The data is then split into:\n",
    "\n",
    "- the training set, which is used to train the model's parameters.\n",
    "- the validation set, which is used to evaluate the capacity of the model to generalize to unseen data.\n",
    "- the test set, which is used to compare different baselines / methods after training.\n",
    "\n",
    "<center><a href=\"https://www.v7labs.com/blog/train-validation-test-set\">\n",
    "    <img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/61568656a13218cdde7f6166_training-data-validation-test.png\" width=\"400\"></a></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the validation set by splitting the training data into 2 subsets (80% training and 20% validation)\n",
    "n_train_examples = int(len(train_data) * 0.8)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "train_data, valid_data = random_split(train_data, [n_train_examples, n_valid_examples])\n",
    "\n",
    "print(len(train_data), len(valid_data))\n",
    "\n",
    "# Define the corresponding dataloaders\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with validation: early stopping\n",
    "\n",
    "A first simple approach to avoid overfitting is to monitor the performance of the model on the validation set over epochs when training. At each epoch we compute the loss on the validation set, and check how it behaves: if the validation loss decreases, then we can continue training. Conversely, if the validation loss increases, it means the model has overfitted, so we need to stop training. In practice, we run training with a specified maximum number of epochs, and we save the model which yields the lowest validation loss. This strategy is called _early stopping_.\n",
    "\n",
    "<center><a href=\"https://theaisummer.com/regularization/\">\n",
    "    <img src=\"https://theaisummer.com/static/7a6353ed78b045f32e4ac39b0b4d66d2/a878e/early-stopping.png\" width=\"400\"></a></center>\n",
    "\n",
    "**Note**: In practice, instead of computing the validation _loss/error_ (which ideally should decrease), we can compute the validation _accuracy_ (which ideally should increase). The concept is the same, but it is usually better to use a validation metric that is as close as possible or equal to the test metric (here: accuracy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def eval_mlp_classifier(model, eval_dataloader):\n",
    "    # Set the model in 'evaluation' mode (this disables some layers (batch norm, dropout...) which are not needed when testing)\n",
    "    model.eval()\n",
    "\n",
    "    # In evaluation phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct_labels = 0\n",
    "        total_labels = 0\n",
    "\n",
    "        # Iterate over the dataset using the dataloader\n",
    "        for images, labels in eval_dataloader:\n",
    "            # Get the predicted labels\n",
    "            images = images.reshape(images.shape[0], -1)\n",
    "            y_predicted = model(images)\n",
    "\n",
    "            # To get the predicted labels, we need to get the max over all possible classes\n",
    "            _, label_predicted = torch.max(y_predicted.data, 1)\n",
    "\n",
    "            # Compute accuracy: count the total number of samples, and the correct labels (compare the true and predicted labels)\n",
    "            total_labels += labels.size(0)\n",
    "            correct_labels += (label_predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct_labels / total_labels\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function is very similar to what we did in the previous lab (so you can reuse it). However, note that here we use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer, which is a more efficient algorithm than SGD (the pytorch function is very similar though: you just need to provide the model's parameters and the learning rate).\n",
    "\n",
    "Then, at the end of each epoch, compute the accuracy on the validation set (use the provided eval function): if the accuracy is increasing, then store the current model as the optimal one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the training function 'train_val_mlp_classifier' with validation\n",
    "def train_val_mlp_classifier(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    input_size,\n",
    "    valid_dataloader,\n",
    "    num_epochs,\n",
    "    loss_fn,\n",
    "    learning_rate,\n",
    "    verbose=True,\n",
    "):\n",
    "    # Make a copy of the model (avoid changing the model outside this function)\n",
    "    model_tr = copy.deepcopy(model)\n",
    "\n",
    "    # Set the model in 'training' mode (ensures all parameters' gradients are computed - it's like setting 'requires_grad=True' for all parameters)\n",
    "    model_tr.train()\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize a list for storing the training loss over epochs\n",
    "    train_losses = []\n",
    "\n",
    "    accuracy_max = 0\n",
    "    val_accuracies = []\n",
    "    model_opt = copy.deepcopy(model_tr)  # Initialize model_opt with the initial model\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize the training loss for the current epoch\n",
    "        tr_loss = 0\n",
    "\n",
    "        # Iterate over batches using the dataloader\n",
    "        for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "            # TO DO: write the training procedure for each batch. This should consist of:\n",
    "            # - vectorizing the images (size should be (batch_size, input_size))\n",
    "            images = images.view(batch_size, input_size)\n",
    "\n",
    "            # - calculate the predicted labels from the vectorized images using 'model_tr'\n",
    "            labels_pred = model_tr(images)\n",
    "\n",
    "            # - using loss_fn, calculate the 'loss' between the predicted and true labels\n",
    "            loss = loss_fn(labels_pred, labels)\n",
    "\n",
    "            # - set the optimizer gradients at 0 for safety\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # - compute the gradients (use the 'backward' method on 'loss')\n",
    "            loss.backward()\n",
    "\n",
    "            # - apply the gradient descent algorithm (perform a step of the optimizer)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the current epoch loss\n",
    "            # Note that 'loss.item()' is the loss averaged over the batch, so multiply it with the current batch size to get the total batch loss\n",
    "            tr_loss += loss.item() * images.shape[0]\n",
    "\n",
    "        # At the end of each epoch, get the average training loss and store it\n",
    "        tr_loss = tr_loss / len(train_dataloader.dataset)\n",
    "        train_losses.append(tr_loss)\n",
    "\n",
    "        # Display the training loss\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"Epoch [{}/{}], Training loss: {:.4f}\".format(\n",
    "                    epoch + 1, num_epochs, tr_loss\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        accuracy = eval_mlp_classifier(model_tr, valid_dataloader)\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        # Store the validation accuracy\n",
    "        if accuracy > accuracy_max:\n",
    "            accuracy_max = accuracy\n",
    "            model_opt = copy.deepcopy(model_tr)\n",
    "        if verbose:\n",
    "            print(\"Validation accuracy: {:.2f}%\\n\".format(accuracy))\n",
    "\n",
    "    return model_opt, train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the network and initialize the parameters for reproducibility\n",
    "input_size = train_data[0][0][0].shape[0] * train_data[0][0][0].shape[1]\n",
    "hidden_size = 8\n",
    "output_size = num_classes\n",
    "model = MLPClassif(input_size, hidden_size, output_size)\n",
    "torch.manual_seed(0)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Training\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "model_opt, train_losses, val_accuracies = train_val_mlp_classifier(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    input_size,\n",
    "    valid_dataloader,\n",
    "    num_epochs,\n",
    "    loss_fn,\n",
    "    learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: plot the training loss and validation accuracy (on two different subplots)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies, color=\"orange\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Validation Accuracy over Epochs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q1**</span> Put the plot above (training loss and validation accuracy) in your report. What do you remark? Is it beneficial to do early stopping?\n",
    "\n",
    "**Note**: Here, we actually conducted training for all epochs, and we simply recorded the best performing model. Proper early stopping would mean that we interrupt training when no improvement is observed after the best validation loss has been obtained (this number of epochs is called a _patience_ parameter, since it corresponds to the \"waiting time\" after the best validation score is obtained).\n",
    "This allows to reduce the computational time in addition to yielding the best model.\n",
    "\n",
    "## Adjusting the model's capacity\n",
    "\n",
    "The number of parameters in the network (i.e., the model's _capacity_) is expected to have a major impact on performance. Too few parameters might yield bad performance (underfitting), while too many might hamper the ability of the network to generalize (overfitting).\n",
    "\n",
    "<center><a href=\"https://classic.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html\">\n",
    "    <img src=\"https://classic.d2l.ai/_images/capacity-vs-error.svg\" width=\"400\"></a></center>\n",
    "\n",
    "Here, we study it by varying the number of hidden layers and checking how the validation accuracy behaves.\n",
    "\n",
    "Below we consider a general classifier module, where the number of hidden layers is passed as an input parameter. To create and stack together several layers, we can use the [ModuleList](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html) object, along with a simple loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On this example, we stack 5 {Linear + Relu} layers\n",
    "list_layer = nn.ModuleList(\n",
    "    [nn.Sequential(nn.Linear(10, 10), nn.ReLU()) for layer in range(5)]\n",
    ")\n",
    "print(list_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the MLP classifier'MLPClassif_var' (with '__init__' and 'forward' methods) with a variable number of hidden layers.\n",
    "class MLPClassif_var(nn.Module):\n",
    "    def __init__(self, hidden_layers_number, input_size, hidden_size, output_size):\n",
    "        super(MLPClassif_var, self).__init__()\n",
    "        self.input_layer = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU())\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.ReLU())\n",
    "                for layer in range(hidden_layers_number)\n",
    "            ]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.input_layer(x)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            y = hidden_layer(y)\n",
    "        out = self.output_layer(y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - Define the general parameters of the MLP classifier model and training (same as before)\n",
    "\n",
    "input_size = train_data[0][0][0].shape[0] * train_data[0][0][0].shape[1]\n",
    "hidden_size = 8\n",
    "output_size = num_classes\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# - Define the number of hidden layers\n",
    "hidden_layers_list = [1, 3, 5, 8]\n",
    "max_accuracies = []\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# - For a number of hidden layers equal to 1, 3, and 5:\n",
    "for hidden_layers_number in hidden_layers_list:\n",
    "    model = MLPClassif_var(hidden_layers_number, input_size, hidden_size, output_size)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    # - Train the model\n",
    "    model_opt, train_losses, val_accuracies = train_val_mlp_classifier(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        input_size,\n",
    "        valid_dataloader,\n",
    "        num_epochs,\n",
    "        loss_fn,\n",
    "        learning_rate,\n",
    "    )\n",
    "\n",
    "    # - Get the maximum validation accuracy\n",
    "    max_accuracies.append(max(val_accuracies))\n",
    "\n",
    "# - Plot the max accuracy as a function of the number of hidden layers\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(hidden_layers_list, max_accuracies, marker=\"o\")\n",
    "plt.xticks(hidden_layers_list)\n",
    "plt.xlabel(\"Number of Hidden Layers\")\n",
    "plt.ylabel(\"Max Validation Accuracy (%)\")\n",
    "plt.title(\"Max Validation Accuracy vs Number of Hidden Layers\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q2**</span> Put the plot above (test accuracy as a function of number of hidden layers) in your report. What do you observe? What's the best model / number of hidden layers to use?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
