{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP autoencoders\n",
    "\n",
    "An autoencoder is a network which consists of two main parts:\n",
    "\n",
    "- an _encoder_, which projects the data into a latent space to transform it into a compact representation.\n",
    "- a _decoder_, which reconstructs the input data from the latent representation.\n",
    "\n",
    "In mathematical terms, a data point $\\mathbf{x} \\in \\mathbb{R}^N$ is transformed into a latent representation $\\mathbf{z} \\in \\mathbb{R}^L$, where $L \\ll N$. Then, the latent representation is passed to the decoder, which produces an approximation $\\hat{\\mathbf{x}} \\in \\mathbb{R}^N$ of the input data, i.e., such that $\\hat{\\mathbf{x}} \\approx \\mathbf{x}$.\n",
    "\n",
    "Autoencoders are very useful in many applications. For instance, in image processing, they are used for image denoising, compression, and generative models (image synthesis and transformation). They can also be used for transfer learning: first an autoencoder is trained to learn a latent representation of the data, and then this representation can be used for other classification/regression tasks.\n",
    "\n",
    "<center><a href=\"https://emkademy.medium.com/1-first-step-to-generative-deep-learning-with-autoencoders-22bd41e56d18\">\n",
    "    <img src=\"https://miro.medium.com/max/772/1*ztZn098tDQsnD5J6v1eNuQ.png\" width=\"600\"></a></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b253749cf543ecbd8eb37543782880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed875d6fd684d64b2b746401da35d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa3562f39d94ae2af5e410c81f5d1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352eda379f1a4754ba6fb325a357e479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Define the data repository\n",
    "data_dir = \"data/\"\n",
    "\n",
    "# Load the MNIST dataset\n",
    "data_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    data_dir, train=True, download=True, transform=data_transforms\n",
    ")\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    data_dir, train=False, download=True, transform=data_transforms\n",
    ")\n",
    "num_classes = len(train_data.classes)\n",
    "\n",
    "# Take a subset of the train/test data\n",
    "train_data = Subset(train_data, torch.arange(400))\n",
    "\n",
    "# We define the train and validation sets and dataloaders as in the previous script\n",
    "n_train_examples = int(len(train_data) * 0.8)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "train_data, valid_data = random_split(train_data, [n_train_examples, n_valid_examples])\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images from the train dataloader\n",
    "batch_example = next(iter(train_dataloader))\n",
    "image_batch_example = batch_example[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The encoder\n",
    "\n",
    "First, let us write the encoder. We consider a 3-layer encoder, where each layer consists of a Linear part and a ReLU activation function. The first layer goes from size `input_size` to 128, the second layer from 128 to 64, and the third layer from 64 to 32.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the encoder class ('__init__' and 'forward' methods)\n",
    "\n",
    "\n",
    "class MLPencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLPencoder, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - instanciate an encoder (get the proper input size)\n",
    "# - vectorize image_batch_example into image_batch_example_vec\n",
    "# - apply the encoder to image_batch_example_vec to produce the latent representation 'z'\n",
    "# - print the size of z, and the size of the image_batch_example_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the latent representation has a significantly smaller dimension than the original data (vectorized image).\n",
    "\n",
    "## The decoder\n",
    "\n",
    "The decoder as a similar structure than the encoder (3 {Linear + activation} layers) but the sizes are flipped: the first layer goes from 32 to 64, the second layer goes from 64 to 128, and the last layer goest from 128 back to the input size. The first and second layers use a ReLU activation, but the last layer uses a Sigmoid: this forces the output to be in the range $[0,1]$, which corresponds to the normalized images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the decoder class ('__init__' and 'forward' methods)\n",
    "\n",
    "\n",
    "class MLPdecoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLPdecoder, self).__init__()\n",
    "\n",
    "    def forward(self, z):\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - instanciate a decoder\n",
    "# - apply it to the latent representation z computed before\n",
    "# - print the size of the output 'y' of the decoder : it should be the same as the input data 'image_batch_example_vec'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The autoencoder main module\n",
    "\n",
    "Finally we can write the autoencoder module: it consists of the encoder and the decoder applied sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the MLP autoencoder class using the previously written encoder and decoder classes\n",
    "class MLPAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLPAutoencoder, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Instanciate an MLP autoencoder and print the number of parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization (ensure reproducibility: everybody should have the same results)\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "    return\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q3**</span> How many parameters are in the autoencoder?\n",
    "\n",
    "## Training\n",
    "\n",
    "Now we can write the training function (with validation !). It's very similar to the training function for the MLP classifier from the previous script, up to two main differences:\n",
    "\n",
    "- since we don't try to predict a label, we don't need to load them when iterating over the dataloader.\n",
    "- the loss function is no longer Cross Entropy (which is for classification), but [MSE](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html?highlight=mse#torch.nn.MSELoss) (_cf._ lab 2).\n",
    "\n",
    "Regarding the loss, remember that the autoencoder tries to compress the input data (through the encoder) and then produce an approximation of this input data (through the decoder). This means that the input and output of the decoder should have the same dimension, and the loss is computed between these two quantities: $\\mathcal{L}(\\hat{\\mathbf{x}}, \\mathbf{x})$.\n",
    "\n",
    "Finally, unlike in lab 4.1, since here we evaluate the model on the validation set by computing the loss (and not the accuracy), be careful that it should be decreasing (instead of increasing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: write the autoencoder training function with validation (also write the evaluation function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - Train the autoencoder: 50 epochs, learning rate = 0.001, and MSE loss function\n",
    "# - After training, save the model parameters and plot the loss over epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the autoencoder to the image_batch_example\n",
    "predicted_batch_example = model_tr(image_batch_example_vec).detach()\n",
    "\n",
    "# Reshape it as a black-and-white image (3D tensor)\n",
    "predicted_batch_example = predicted_batch_example.reshape(batch_size, 1, 28, 28)\n",
    "\n",
    "# Plot the original and predicted images\n",
    "for ib in range(batch_size):\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_batch_example[ib, :].squeeze(), cmap=\"gray_r\")\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title(\"Original image\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(predicted_batch_example[ib, :].squeeze(), cmap=\"gray_r\")\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title(\"Predicted image\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q4**</span> In your report, put the plot with the training/validation losses, and one of the plots above (one original image and its corresponding estimation).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
