{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification with multilayer perceptron (MLP)\n",
    "\n",
    "In this lab we design and train our first MLP network, and we use it for image classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In this lab, we use the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset, which is a large image dataset of hand-written digits.\n",
    "\n",
    "<center><a href=\"https://en.wikipedia.org/wiki/MNIST_database\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"></a></center>\n",
    "\n",
    "Just like some other widely-used datasets, MNIST can be downloaded directly from Pytorch, and includes specific commands to create a `Dataset` object, thus you don't have to do it manually as we did in lab 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data repository\n",
    "data_dir = \"../data/\"\n",
    "\n",
    "# Choose one (or several) transform(s) to preprocess the data\n",
    "# Here, we transform it to torch tensors, and we normalize the data (to zero-mean and unit-variance)\n",
    "data_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a Dataset (you can download the data by setting 'download=True')\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    data_dir, train=True, download=True, transform=data_transforms\n",
    ")\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    data_dir, train=False, download=True, transform=data_transforms\n",
    ")\n",
    "num_classes = len(train_data.classes)\n",
    "print(\"Number of classes in the train dataset:\", num_classes)\n",
    "\n",
    "# We are not going to work with the full dataset (which is very big), so we only keep small train and test subsets.\n",
    "train_data = Subset(train_data, torch.arange(400).tolist())\n",
    "test_data = Subset(test_data, torch.arange(50).tolist())\n",
    "print(\"Number of images in the train dataset:\", len(train_data))\n",
    "print(\"Number of images in the test dataset:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one data pair (one image and the corresponding label)\n",
    "image, label = train_data[0]  # type: ignore\n",
    "print(image.shape)\n",
    "print(\"Image label:\", label)\n",
    "\n",
    "# You should note that the size of the image is [1, 28, 28], which corresponds to [num_channels, height, width]\n",
    "# Indeed, MNIST images are in black and white so there is only 1 color channel.\n",
    "# To plot this image, we need to remove this channel dimension by using squeeze()\n",
    "plt.figure()\n",
    "plt.imshow(image.squeeze(), cmap=\"gray_r\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - create two dataloaders (for the training and testing subsets) with a batch size of 8\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "# - print the number of batches in the training subset\n",
    "num_batches = len(train_dataloader)\n",
    "print(\"Number of batches in the training subset:\", num_batches)\n",
    "\n",
    "# - print the number of batches in the testing subset\n",
    "num_batches = len(test_dataloader)\n",
    "print(\"Number of batches in the testing subset:\", num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images and corresponding labels from the train dataloader\n",
    "batch_example = next(iter(train_dataloader))\n",
    "images_batch_example = batch_example[0]\n",
    "labels_batch_example = batch_example[1]\n",
    "\n",
    "# Print the size of the batch of images and labels\n",
    "print(images_batch_example.shape)\n",
    "print(labels_batch_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the images in the batch (along with the corresponding label)\n",
    "plt.figure(figsize=(15, 5))\n",
    "for ib in range(batch_size):\n",
    "    plt.subplot(batch_size // 4, 4, ib + 1)\n",
    "    plt.imshow(images_batch_example[ib, :].squeeze(), cmap=\"gray_r\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Image label = \" + str(labels_batch_example[ib].item()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP network\n",
    "\n",
    "A typical MLP network is composed of several layers:\n",
    "\n",
    "- an _input_ layer, which takes a batch of vectors and computes the first hidden representation.\n",
    "- one or several _hidden_ layers.\n",
    "- an _output_ layer, which computes the output of the network.\n",
    "\n",
    "Each layer consists of a linear part and a non-linear _activation_ functions (except for the output layer, which usually don't use an activation function). There are many non-linear activation functions in Pytorch, check the [documentation](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) for a complete overview. A network is said to be _deep_ if it has at least 3 layers (input, at least one hidden, and output).\n",
    "\n",
    "<center><a href=\"https://www.researchgate.net/publication/309592737_Classification_of_VoIP_and_non-VoIP_traffic_using_machine_learning_approaches\">\n",
    "    <img src=\"https://www.researchgate.net/profile/Mouhammd-Alkasassbeh/publication/309592737/figure/fig2/AS:423712664100865@1478032379613/MultiLayer-Perceptron-MLP-sturcture-334-MultiLayer-Perceptron-Classifier-MultiLayer.png\"></a></center>\n",
    "\n",
    "**Note**: Since an MLP manipulates vectors (= 1D-tensors) as inputs, in image processing we first have to transform our images into vectors. For instance, if a black-and-white image is a (3D) tensor of size `[1, 28, 28]`, then we have to reshape it into a tensor of size `[1x28x28] = [784]`.\n",
    "\n",
    "### Example\n",
    "\n",
    "First, we will write a succession of operations which correspond to applying the MLP classifier on the example batch `image_batch_example`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: reshape the image_batch_example into a batch of vectors (vectorized images):\n",
    "# 'image_batch_example' has size [batch_size, 1, 28, 28]\n",
    "# 'vectorized_batch' should have size [batch_size, 1*28*28]\n",
    "\n",
    "# - use the function view() to reshape the batch of images\n",
    "vectorized_batch = images_batch_example.view(batch_size, -1)\n",
    "\n",
    "# - print the size of the vectorized batch\n",
    "print(\"Size of the vectorized batch:\", vectorized_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create the input layer (linear and activation) and we pass the vectorized batch to it\n",
    "input_size = vectorized_batch.shape[-1]\n",
    "hidden_size = 10\n",
    "\n",
    "input_layer = nn.Sequential(nn.Linear(input_size, hidden_size), nn.Sigmoid())\n",
    "y = input_layer(vectorized_batch)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: create the two other layers (hidden_layer and output_layer):\n",
    "# - the hidden layer goes from 'hidden_size' to 'hidden_size', and uses a Sigmoid activation function\n",
    "hidden_layer = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.Sigmoid())\n",
    "\n",
    "# - the output layer goes from 'hidden_size' to 'output_size', which is the number of classes in the dataset (it uses no activation function)\n",
    "output_size = 10  # (because we have 10 classes in the MNIST dataset for digits 0-9)\n",
    "output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "# - Compute z = hidden_layer(y) and out=output_layer(z). Print the size of 'out'.\n",
    "z = hidden_layer(y)\n",
    "out = output_layer(z)\n",
    "\n",
    "print(\"Size of the output:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we have done above, two important remarks can be made:\n",
    "\n",
    "- The output `out` has size `[batch_size, num_classes]` while the true labels `labels_batch_example` has size `[batch_size]`. This is because `out` contains a predicted probability for each class, while `labels_batch_example` simply contains the true labels.\n",
    "- In classification tasks, we want to output probabilities per class. However, nothing ensures that `out` corresponds to probabilities, since it is not normalized and we didn't use any output activation function (values can be negative and not sum up to 1).\n",
    "\n",
    "However, when training a classification network, we generally use the [Cross Entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) loss function, which alleviates these two issues. This loss is optimized for handling true labels instead of true probabilities per class, so you don't have to worry about it. Besides, it will automatically apply a [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) non-linearity to the predicted outputs, in order to normalize them as probabilities per class.\n",
    "\n",
    "**Note**: Instead of Cross Entropy, you can use the [Negative log-likelihood](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) similarly. It will also solve the first problem, but then you need to manually add a [log Softmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax) as output activation to normalize the outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Cross Entropy as loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute the error between the predicted labels 'out' and true labels 'labels_batch_example'\n",
    "loss_batch = loss_fn(out, labels_batch_example)\n",
    "print(loss_batch.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General module\n",
    "\n",
    "Now, let's create a general MLP classification network. It's a python class that inherits from the general `nn.Module` object, and it should contain at least 2 methods:\n",
    "\n",
    "- `__init__`, which initializes the network when instanciated (creates all the layers and stores some useful parameters if needed).\n",
    "- `forward`, which applies the forward pass (i.e., compute the output 'out' from the input and using the layers).\n",
    "\n",
    "You can add other methods if needed but these two are sufficient for now.\n",
    "\n",
    "**Note**: Remember that Python classes usually define and use some variables/data/tensors/dictionary etc. internally (this includes network layers) called _attributes_: they should be defined in the `__init__` method with a specific structure (the name should start by `self.`, as we did in lab 1). This allows you to access these attributes in other methods, or after defining your network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassif(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, act_fn):\n",
    "        super(MLPClassif, self).__init__()\n",
    "\n",
    "        # TO DO: define the input, hidden, and output layers as before\n",
    "        self.input_layer = nn.Sequential(nn.Linear(input_size, hidden_size), act_fn)\n",
    "        self.hidden_layer = nn.Sequential(nn.Linear(hidden_size, hidden_size), act_fn)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TO DO: write the 'forward' method, which computes the output 'out' from the input x\n",
    "        # It should apply sequentially the input, hidden, and output layer, as we did in the example before.\n",
    "        y = self.input_layer(x)\n",
    "        z = self.hidden_layer(y)\n",
    "        out = self.output_layer(z)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To DO: Instanciate an MLP classifier called 'model' with a hidden size of 10 and a Sigmoid activation function\n",
    "model = MLPClassif(input_size, hidden_size, output_size, act_fn=nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization (ensure reproducibility: everybody should have the same results)\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "    return\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can save / load the model's parameters as follow:\n",
    "torch.save(model.state_dict(), \"model_mlp_classif.pt\")\n",
    "model.load_state_dict(torch.load(\"model_mlp_classif.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A useful command to get the total number of parameters in the model\n",
    "print(\"Total number of parameters:\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q1**</span> How many parameters are in the network?\n",
    "\n",
    "## Training\n",
    "\n",
    "We now write the function for training the network. It's very similar to what we did in lab 2, except now we process batches of data instead of the whole dataset at each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_mlp_classifier(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    loss_fn,\n",
    "    learning_rate: float,\n",
    "    verbose=True,\n",
    "):\n",
    "    # Make a copy of the model (avoid changing the model outside this function)\n",
    "    model_tr = copy.deepcopy(model)\n",
    "\n",
    "    # Set the model in 'training' mode (ensures all parameters' gradients are computed - it's like setting 'requires_grad=True' for all parameters)\n",
    "    model_tr.train()\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.SGD(model_tr.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize a list for storing the training loss over epochs\n",
    "    train_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize the training loss for the current epoch\n",
    "        tr_loss = 0\n",
    "\n",
    "        # Iterate over batches using the dataloader\n",
    "        for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "            # TO DO: write the training procedure for each batch. This should consist of:\n",
    "            # - vectorizing the images (size should be (batch_size, input_size))\n",
    "            images = images.view(batch_size, -1)\n",
    "\n",
    "            # - calculate the predicted labels from the vectorized images using 'model_tr'\n",
    "            labels_pred = model_tr(images)\n",
    "\n",
    "            # - using loss_fn, calculate the 'loss' between the predicted and true labels\n",
    "            loss = loss_fn(labels_pred, labels)\n",
    "\n",
    "            # - set the optimizer gradients at 0 for safety\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # - compute the gradients (use the 'backward' method on 'loss')\n",
    "            loss.backward()\n",
    "\n",
    "            # - apply the gradient descent algorithm (perform a step of the optimizer)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the current epoch loss\n",
    "            # Note that 'loss.item()' is the loss averaged over the batch, so multiply it with the current batch size to get the total batch loss\n",
    "            tr_loss += loss.item() * batch_size\n",
    "\n",
    "        # At the end of each epoch, get the average training loss and store it\n",
    "        tr_loss = tr_loss / (len(train_dataloader) * batch_size)\n",
    "        train_losses.append(tr_loss)\n",
    "\n",
    "        # Display the training loss\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"Epoch [{}/{}], Training loss: {:.4f}\".format(\n",
    "                    epoch + 1, num_epochs, tr_loss\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return model_tr, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define the training parameters and train the model\n",
    "# - 30 epochs\n",
    "# - learning rate = 0.01\n",
    "# - loss function: Cross Entropy\n",
    "# After training, save the model parameters and display the loss over epochs\n",
    "\n",
    "num_epochs = 30\n",
    "learning_rate = 0.01\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model_trained, train_losses = training_mlp_classifier(\n",
    "    model, train_dataloader, num_epochs, loss_fn, learning_rate, verbose=True\n",
    ")\n",
    "\n",
    "# Save the model parameters\n",
    "torch.save(model_trained.state_dict(), \"model_mlp_classif_trained.pt\")\n",
    "\n",
    "# Plot the training loss over epochs\n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Training loss over epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now the model is trained, we can evaluate it on the test dataset. We do that by predicting the labels using our model, and comparing it with the true labels. This allows us to compute the classification accuracy, which is provided in the function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function: similar to the training loop, except we don't need to compute any gradient / backprop\n",
    "def eval_mlp_classifier(model: nn.Module, eval_dataloader: DataLoader):\n",
    "    # Set the model in 'evaluation' mode (this disables some layers (batch norm, dropout...) which are not needed when testing)\n",
    "    model.eval()\n",
    "\n",
    "    # In evaluation phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct_labels = 0\n",
    "        total_labels = 0\n",
    "\n",
    "        # Iterate over the dataset using the dataloader\n",
    "        for images, labels in eval_dataloader:\n",
    "            # Get the predicted labels\n",
    "            images = images.view(batch_size, -1)\n",
    "            y_predicted = model(images)\n",
    "\n",
    "            # To get the predicted labels, we need to get the max over all possible classes\n",
    "            _, labels_predicted = torch.max(y_predicted.data, 1)\n",
    "\n",
    "            # Compute accuracy: count the total number of samples, and the correct labels (compare the true and predicted labels)\n",
    "            total_labels += labels.size(0)\n",
    "            correct_labels += (labels_predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct_labels / total_labels\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Evaluate the model on the test set:\n",
    "# - Instanciate an MLP newtork and load the trained parameters\n",
    "model_test = MLPClassif(input_size, hidden_size, output_size, act_fn=nn.Sigmoid())\n",
    "model_test.load_state_dict(torch.load(\"model_mlp_classif_trained.pt\"))\n",
    "\n",
    "# - Apply the evaluation function using the test dataloader\n",
    "test_accuracy = eval_mlp_classifier(model_test, test_dataloader)\n",
    "\n",
    "# - Print the test accuracy\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_accuracy))\n",
    "\n",
    "# - Plot the training loss over epochs\n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Training loss over epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q2**</span> Put the plot above (training loss over epochs) and the test accuracy in your report. Based on the plot, can you suggest a way to improve the test accuracy?\n",
    "\n",
    "## Influence of the activation function\n",
    "\n",
    "We used the MLP classifier with a Sigmoid activation function, but another common choice is the [Rectified Linear Unit](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU). Here we investigate how it performs compared to the Sigmoid-based network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define an MLP classifier called 'model_relu' using a ReLU activation.\n",
    "model_relu = MLPClassif(input_size, hidden_size, output_size, act_fn=nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights (for reproducibility)\n",
    "torch.manual_seed(0)\n",
    "model_relu.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Train the network and evaluate it.\n",
    "model_relu_trained, train_losses_relu = training_mlp_classifier(\n",
    "    model_relu, train_dataloader, num_epochs, loss_fn, learning_rate, verbose=True\n",
    ")\n",
    "\n",
    "# Training, plot the loss and save the model's parameters\n",
    "plt.figure()\n",
    "plt.plot(train_losses_relu)\n",
    "plt.title(\"Training loss over epochs (ReLU)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "torch.save(model_relu_trained.state_dict(), \"model_relu_mlp_classif_trained.pt\")\n",
    "\n",
    "# Evaluate on the test set and print accuracy\n",
    "model_relu_test = MLPClassif(input_size, hidden_size, output_size, act_fn=nn.ReLU())\n",
    "model_relu_test.load_state_dict(torch.load(\"model_relu_mlp_classif_trained.pt\"))\n",
    "\n",
    "test_accuracy_relu = eval_mlp_classifier(model_relu_test, test_dataloader)\n",
    "print(\"Test accuracy (ReLU): {:.2f}%\".format(test_accuracy_relu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q3**</span> What is the accuracy on the test set with this network? Which one (Sigmoid or ReLU) would you recommend to use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the model capacity\n",
    "\n",
    "In order to improve performance, a straightforward approach is to increase the model capacity, i.e., increase the number of parameters. There are basically two ways to do so: either increase the number of neurons in each layer (_width_) of increase the total number of layers (_depth_). Let's focus here on width.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: define, train and evalute an MLP classifier model with a variable hidden_size:\n",
    "# - 'hidden_size' ranges in [3, 5, 10, 50]\n",
    "# - the networks use ReLU activation, and training uses 30 epochs\n",
    "# - for each hidden size, print the number of parameters and the test accuracy\n",
    "# Remember to initialize the weights of the network after instanciating it for reproducibility.\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "# hidden_sizes = np.arange(1, 50)  # Hidden sizes from 1 to 50 with step of 5\n",
    "hidden_sizes = [3, 5, 10, 50]\n",
    "test_accuracies = []  # Initialize a list to store test accuracies\n",
    "\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    # Create the model with the current hidden size\n",
    "    model_var = MLPClassif(input_size, hidden_size, output_size, act_fn=nn.ReLU())\n",
    "\n",
    "    # Initialize the weights\n",
    "    torch.manual_seed(0)\n",
    "    model_var.apply(init_weights)\n",
    "\n",
    "    # Train the model\n",
    "    model_var_trained, train_losses_var = training_mlp_classifier(\n",
    "        model_var, train_dataloader, num_epochs, loss_fn, learning_rate, verbose=False\n",
    "    )\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    model_var_test = MLPClassif(input_size, hidden_size, output_size, act_fn=nn.ReLU())\n",
    "    model_var_test.load_state_dict(model_var_trained.state_dict())\n",
    "\n",
    "    test_accuracy_var = eval_mlp_classifier(model_var_test, test_dataloader)\n",
    "    test_accuracies.append(test_accuracy_var)  # Append the test accuracy to the list\n",
    "\n",
    "    # Print the number of parameters and test accuracy\n",
    "    num_params = sum(p.numel() for p in model_var.parameters())\n",
    "    print(\n",
    "        \"Hidden size: {}, Number of parameters: {}, Test accuracy: {:.2f}%\".format(\n",
    "            hidden_size, num_params, test_accuracy_var\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Report the test accuracy as a function of hidden_size\n",
    "plt.figure()\n",
    "plt.plot(hidden_sizes, test_accuracies, marker=\"o\")\n",
    "plt.title(\"Test accuracy as a function of hidden size\")\n",
    "plt.xlabel(\"Hidden size\")\n",
    "plt.ylabel(\"Test accuracy (%)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q4**</span> Report the test accuracy as a function of `hidden_size`. Which value of `hidden_size` would you use and why?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
