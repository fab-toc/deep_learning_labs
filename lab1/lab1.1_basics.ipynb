{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch basics\n",
    "\n",
    "<center><a href=\"https://pytorch.org/\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png\" width=\"300\"></a></center>\n",
    "    \n",
    "In this first script, we see the basics of Pytorch for manipulating tensors.\n",
    "\n",
    "Remember to check the [official documentation](https://pytorch.org/docs/stable/index.html), which is frequently used in these labs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tensors\n",
    "\n",
    "A [tensor](https://en.wikipedia.org/wiki/Tensor) is a multi-dimensional array (just like numpy arrays), which can store any structured numerical data. A 0D-tensor is just a scalar number, a 1D-tensor is a vector, a 2D-tensor is a matrix, a 3D-tensor is a \"cube\", etc.\n",
    "\n",
    "<center><a href=\"https://medium.com/@anoorasfatima/10-most-common-maths-operation-with-pytorchs-tensor-70a491d8cafd\">\n",
    "    <img src=\"https://miro.medium.com/max/1308/1*8jdzMrA33Leu3j3F6A8a3w.png\" width=\"400\"></a></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder of numpy\n",
    "x = np.zeros((1, 5))  # create a 2D array (of shape 1x5) filled with 0\n",
    "print(x)\n",
    "x = np.ones((1, 5))  # create a 2D array (of shape 1x5) filled with 1\n",
    "print(x)\n",
    "x = np.array([[1, 2], [3, 4]])  # create an array with specified values\n",
    "print(x)\n",
    "print(x.shape, x.shape[0], x.shape[1])  # get the shape of an array\n",
    "x = np.random.randn(\n",
    "    4, 5\n",
    ")  # create an array with numbers drawn from a standard normal distribution\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: create the same tensors in pytorch\n",
    "# hint: use 'torch' instead of 'np', the functions are similar\n",
    "\n",
    "x = torch.zeros((1, 5))  # create a 2D array (of shape 1x5) filled with 0\n",
    "print(\"x: \", x)\n",
    "\n",
    "x = torch.ones((1, 5))  # create a 2D array (of shape 1x5) filled with 1\n",
    "print(\"x: \", x)\n",
    "\n",
    "x = torch.asarray([[1, 2], [3, 4]])  # create an array with specified values\n",
    "print(\"x: \", x)\n",
    "\n",
    "print(x.shape, x.shape[0], x.shape[1])  # get the shape of an array\n",
    "\n",
    "x = torch.randn(\n",
    "    4, 5\n",
    ")  # create an array with numbers drawn from a standard normal distribution\n",
    "print(\"x: \", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy Bridge: it's also possible to directly transform numpy arrays into pytorch tensor\n",
    "x_np = np.ones((2, 3))\n",
    "print(x_np)\n",
    "x_pt = torch.from_numpy(x_np)\n",
    "print(x_pt)\n",
    "\n",
    "# And conversely:\n",
    "x_np = x_pt.numpy()\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to create a tensor filled with a given value\n",
    "x = torch.zeros((1, 5))\n",
    "x.fill_(20)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With some functions, you can create an array without explicitly providing the shape (but instead use another tensor)\n",
    "y = torch.randn_like(x)\n",
    "print(y)\n",
    "print(y.shape, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - create a tensor x0 that has the same size of x and filled with 0\n",
    "x0 = torch.zeros_like(x)\n",
    "\n",
    "# - create a tensor x1 that has the same size of x and filled with 1\n",
    "x1 = torch.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of numbers from 'ind_beg' to 'ind_end' with an increment of 'inc_step'\n",
    "ind_beg = 3\n",
    "ind_end = 10\n",
    "inc_step = 2\n",
    "x = torch.arange(ind_beg, ind_end, inc_step)\n",
    "print(x)\n",
    "\n",
    "# Default values are 'ind_beg=0' and 'inc_step=1':\n",
    "x = torch.arange(ind_end)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most simple 'for' loop\n",
    "for i in range(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, you can create a tensor containing the indices over which iterating\n",
    "list_iter = torch.arange(10)\n",
    "for i in list_iter:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also directly iterate over the elements of a 1D-tensor (or list)\n",
    "my_list = torch.randn(10)\n",
    "print(my_list)\n",
    "for x in my_list:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use 'enumerate', you can keep track of the index\n",
    "for i, x in enumerate(my_list):\n",
    "    print(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the object to iterate is a multivariate tensor, then it will iterate over the first dimension\n",
    "# For instance, if we iterate over a tensor of size [10, 16, 16], it will produce 10 tensors of size [16,16]\n",
    "\n",
    "mytensor = torch.randn(10, 16, 16)\n",
    "for x in mytensor:\n",
    "    print(x.shape)\n",
    "    plt.figure()\n",
    "    plt.imshow(x)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Basic operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((1, 5))\n",
    "x.fill_(5)\n",
    "y = torch.ones((1, 5))\n",
    "y.fill_(3)\n",
    "\n",
    "# Addition, subtraction, multiplication, division, and power\n",
    "print(x + y)\n",
    "print(x - y)\n",
    "print(x * y)\n",
    "print(x / y)\n",
    "print(x**y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch has some built-in basic math functions (exp, sin, cos...) that can be applied element-wise to a tensor\n",
    "x = torch.randn(2, 3)\n",
    "y = torch.exp(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: plot the function y=sin(x)\n",
    "# - create a tensor x which ranges from -5 to 5 with a step increment of 0.1\n",
    "x = torch.arange(-5, 5, 0.1)\n",
    "\n",
    "# - compute y=sin(x) (use the torch.sin function)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# - plot it using plt.plot(x, y)\n",
    "plt.plot(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: plot a noisy sinusoid\n",
    "# - create a noise tensor called 'noise' with the same shape as y (use torch.randn_like)\n",
    "noise = torch.randn_like(y)\n",
    "\n",
    "# - create a scalar amount_noise which control the amount of noise (set it at 0.1 for instance)\n",
    "amount_noise = 0.02\n",
    "\n",
    "# - compute z, which is the sum of y and the noise tensor * the amount of noise\n",
    "z = y + noise * amount_noise\n",
    "\n",
    "# - plot z as a function of x\n",
    "plt.plot(x, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing (same as in numpy)\n",
    "x = torch.randn(5, 6)\n",
    "print(x[:3])  # slice over the first dimension\n",
    "print(x[:, :3])  # slice over the second dimension\n",
    "print(x[:3, :3])  # slice over both dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful functions are min, max, argmin and argmax\n",
    "x = torch.rand(1, 5)\n",
    "print(x)\n",
    "print(x.min(), x.max(), x.argmin(), x.argmax())\n",
    "\n",
    "# It's also easy to sort a tensor with ascending values\n",
    "x_sorted, ind_sort = x.sort()\n",
    "print(x_sorted, ind_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: find the minimum of a quadratic function\n",
    "# - create a tensor x which ranges from -4 to 4 with a step of 0.01\n",
    "x = torch.arange(-4, 4, 0.01)\n",
    "\n",
    "# - compute y = x^2 - 5x + 3\n",
    "y = x**2 - 5 * x + 3\n",
    "\n",
    "# - plot y as a function of x\n",
    "plt.plot(x, y)\n",
    "\n",
    "# - compute the minimum value of y and print it\n",
    "print(torch.min(y))\n",
    "\n",
    "# - find the index 'ind_min' corresponding to this minimum (hint: use the 'argmin' function)\n",
    "ind_min = torch.argmin(y)\n",
    "print(ind_min)\n",
    "\n",
    "# - compute the value of x corresponding to the minimum: x[ind_min]\n",
    "x_min = x[ind_min]\n",
    "print(x_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Tensor types\n",
    "\n",
    "In Pytorch there are several data types, which are listed in the [documentation](https://pytorch.org/docs/stable/tensors.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 10)\n",
    "print(x)\n",
    "\n",
    "# Display the type using the 'dtype' attribute\n",
    "# By default, it should be float32\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the type using the 'type' method\n",
    "x = x.type(torch.float16)\n",
    "print(x.dtype)\n",
    "\n",
    "# Convert it to integer\n",
    "x = x.type(torch.int16)\n",
    "print(x.dtype)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can specify the type when creating a tensor\n",
    "x = torch.tensor(3, dtype=torch.int)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it's a float\n",
    "x = torch.tensor(3, dtype=torch.int)\n",
    "print(x.is_floating_point())\n",
    "\n",
    "pi = torch.tensor(3.14159)\n",
    "print(pi, pi.is_floating_point(), pi.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8, 5)\n",
    "print(x.shape)\n",
    "\n",
    "# Transposition: use either 'x.t()' or 'x.transpose(dims)' where 'dims' specifies the new dimensions order\n",
    "y = x.transpose(1, 0)\n",
    "print(y.shape)\n",
    "\n",
    "z = x.t()\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape: reorganize the tensor with the specified output dimensions (similar as 'numpy.reshape')\n",
    "x = torch.randn(8, 5)\n",
    "y = x.reshape(10, 4)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "# You can only specify one dimension and mark the other with '-1', and it will autocomplete consistently\n",
    "z = x.reshape(-1, 10)\n",
    "print(z.shape)\n",
    "z = x.reshape(2, -1)\n",
    "print(z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View: similar as 'reshape', but only creates a view over the tensor: if the original data is changed, then the viewed tensors also changes\n",
    "x = torch.zeros(8, 5)\n",
    "y = x.view(10, 4)\n",
    "print(y)\n",
    "\n",
    "x.fill_(1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze and unsqueeze\n",
    "\n",
    "It's important to understand that there is a difference between a tensor of shape `[a, b]` and a tensor of shape `[a, b, 1]`: even though the same data can be stored in both, the second one as an extra dimension (the third one) that can be useful, e.g., if you want to assemble tensors together in a bigger object.\n",
    "\n",
    "For instance, an image with height `a` and width `b` can be seen as a 2D-tensor of shape `[a, b]`. But you can also view it as a single frame in a video, then it's a 3D-tensor of shape `[a, b, 1]`, and the whole video made up with `c` frames would be a 3D-tensor of shape `[a, b, c]`.\n",
    "\n",
    "From a given tensor, you can add an extra dimension (with shape 1) using the `unsqueeze` method. Conversely, you can remove a dimension with shape 1 using the `squeeze` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of squeeze: remove all dimensions with shape 1\n",
    "x = torch.zeros(2, 1, 5, 1)\n",
    "print(x.shape)\n",
    "y = x.squeeze()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversely, 'unsqueeze' allows you to expand a tensor by adding a new dimension, whose location must be specified\n",
    "x = torch.zeros(2, 5)\n",
    "print(x.shape)\n",
    "y = x.unsqueeze(dim=0)\n",
    "print(y.shape)\n",
    "z = x.unsqueeze(dim=2)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate: useful to concatenate tensors along a specified (existing) dimension\n",
    "# Works with any tensors, provided that the dimensions over which you don't concatenate are consistent\n",
    "x1 = torch.rand(15, 64, 64)\n",
    "x2 = torch.rand(50, 64, 64)\n",
    "X_concat = torch.cat((x1, x2), dim=0)\n",
    "print(X_concat.shape)\n",
    "\n",
    "x1 = torch.rand(10, 217)\n",
    "x2 = torch.rand(10, 489)\n",
    "X_concat = torch.cat((x1, x2), dim=1)\n",
    "print(X_concat.shape)\n",
    "\n",
    "x1 = torch.rand(10, 217, 12)\n",
    "x2 = torch.rand(10, 217, 14)\n",
    "X_concat = torch.cat((x1, x2), dim=2)\n",
    "print(X_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO :\n",
    "# - create two tensors of shape 16x16 with random values.\n",
    "x = torch.rand(16, 16)\n",
    "y = torch.rand(16, 16)\n",
    "\n",
    "# - unsqueeze them to create a new dimension (of size 1)\n",
    "x = x.unsqueeze(dim=0)\n",
    "y = y.unsqueeze(dim=0)\n",
    "\n",
    "# - concatenate them into a single tensor of size (2, 16, 16)\n",
    "X_concat = torch.cat((x, y), dim=0)\n",
    "print(X_concat.shape)\n",
    "\n",
    "# hint: first 'unsqueeze' the tensors to create a new dimension, and then 'cat' over this dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack: unlike 'cat', 'stack' concatenates the tensors along a new dimension (the inputs tensors must have the same shape)\n",
    "x = torch.ones(3, 10)\n",
    "y = torch.ones(3, 10)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "z_stack = torch.stack((x, y), dim=0)\n",
    "print(z_stack.shape)\n",
    "\n",
    "# Check the difference with 'cat'\n",
    "z_cat = torch.cat((x, y), dim=0)\n",
    "print(z_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's possible to stack over any dimension, so it will create a tensor accordingly\n",
    "z_stack = torch.stack((x, y), dim=1)\n",
    "print(z_stack.shape)\n",
    "\n",
    "z_stack = torch.stack((x, y), dim=2)\n",
    "print(z_stack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO : same exercice as before but using stack (should be simpler)\n",
    "# - create two tensors of shape 16x16 with random values.\n",
    "x = torch.rand(16, 16)\n",
    "y = torch.rand(16, 16)\n",
    "\n",
    "# - stack them to create a new dimension (of size 2)\n",
    "z_stack = torch.stack((x, y), dim=0)\n",
    "print(z_stack.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder in numpy\n",
    "x_np = np.ones((2, 3))\n",
    "np_filepath = \"x_np.npy\"\n",
    "np.save(np_filepath, x_np)\n",
    "x_np_load = np.load(np_filepath)\n",
    "print(x_np_load)\n",
    "\n",
    "# In pytorch, it's very similar\n",
    "x_tensor = torch.from_numpy(x_np)\n",
    "tensor_filepath = \"x_tensor.pt\"\n",
    "torch.save(x_tensor, tensor_filepath)\n",
    "x_tensor_load = torch.load(tensor_filepath)\n",
    "print(x_tensor_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Devices\n",
    "\n",
    "Finally, let's note that there are two types of _devices_ in Pytorch: `cpu` and `cuda`. By default, every tensor's device is `cpu`, which means that all computation is performed on the CPU. However, when training big models and handling big datasets (as in deep learning), it's more convenient to use a graphic card (GPU) for the computation. To do that, we just need to tell Pytorch that the data / tensors / models should be copied on a `cuda` device.\n",
    "This is mostly for your general knowledge: we won't use GPU computation in these labs.\n",
    "\n",
    "**Note**: If you didn't install pytorch with CUDA, you should get an error if you try to copy a tensor to `cuda`. That's fine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check if a GPU is available (and how many)\n",
    "print(\"Cuda available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "# By default, any tensor will be on a 'cpu' device\n",
    "x = torch.rand(1, 10)\n",
    "print(x.device)\n",
    "\n",
    "# You can change it using the 'to' method\n",
    "# Doing this is only possible if you have installed CUDA with Pytorch, so you might get an error here.\n",
    "x = x.to(\"cuda\")\n",
    "print(x.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
