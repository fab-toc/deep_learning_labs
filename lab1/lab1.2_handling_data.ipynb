{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling data in PyTorch\n",
    "\n",
    "In this script, we see how to handle data in PyTorch using Datasets and Dataloaders objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import imageio.v2 as io\n",
    "import soundfile as sf\n",
    "import fnmatch\n",
    "from IPython.display import Audio, display\n",
    "from matplotlib import pyplot as plt\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image data\n",
    "\n",
    "First, let's play around with images. We use the data provided in `data/image_corpus`, which is extracted from the [Pokemon dataset](https://www.kaggle.com/datasets/vishalsubbiah/pokemon-images-and-types).\n",
    "\n",
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display one image from the image corpus\n",
    "image_path = \"../data/image_corpus/pikachu.png\"\n",
    "image_example = io.imread(image_path, pilmode=\"RGBA\")\n",
    "plt.figure()\n",
    "plt.imshow(image_example)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PNG images usually have 4 channels (red, green, blue, and transparency)\n",
    "print(image_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get the name of the pokemon by removing 'data/image_corpus/' and '.png' from the image_path\n",
    "name = image_path.replace(\"../data/image_corpus/\", \"\").replace(\".png\", \"\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We now create an image Dataset. It's a python class that allows to store and manipulate all the data instead of manually load and process each data sample (=image) independently.\n",
    "\n",
    "A `Dataset` python class must consists of at least 3 methods:\n",
    "\n",
    "- `__init__`, which initializes the object when instanciated.\n",
    "- `__len__`, which returns the lenght (= number of samples) of the dataset.\n",
    "- `__getitem__`, which allows to access a sample using it's index number.\n",
    "\n",
    "Of course you can add as many methods as you want for preprocessing the data if needed.\n",
    "\n",
    "**Note**: Python classes usually define and use some variables/data/tensors/dictionary etc. internally. These are called _attributes_, and should be defined in the `__init__` method with a specific structure (the name should start by `self.`, see below). This allows you to access these attributes in other methods, or after defining your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        \"\"\"Initialize the attributes of the object of the class.\"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.image_path_list = sorted(self._find_files(image_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the dataset.\"\"\"\n",
    "        return len(self.image_path_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a data sample (=image) for a given index, along with the name of the corresponding pokemon.\"\"\"\n",
    "\n",
    "        # TO DO:\n",
    "        # - get the image path corresponding to 'index' (use the list 'self.image_path_list')\n",
    "        image_path = self.image_path_list[index]\n",
    "\n",
    "        # - get the pokemon name\n",
    "        name = image_path.replace(self.image_dir, \"\").replace(\".png\", \"\")\n",
    "\n",
    "        # - load the image into a numpy array x\n",
    "        x = io.imread(image_path, pilmode=\"RGBA\")\n",
    "\n",
    "        # - transform x into a pytorch tensor of type 'float'\n",
    "        x = torch.from_numpy(x).float()\n",
    "\n",
    "        # - normalize it so x ranges between 0 and 1\n",
    "        x = x / 255.0\n",
    "\n",
    "        # - return the tensor x and the pokemon name\n",
    "        return x, name\n",
    "\n",
    "    def _find_files(self, directory, pattern=\"*.png\"):\n",
    "        \"\"\"Recursively find all files matching the pattern.\"\"\"\n",
    "        image_path_list = []\n",
    "        for root, dirnames, filenames in os.walk(directory):\n",
    "            for filename in fnmatch.filter(filenames, pattern):\n",
    "                image_path_list.append(os.path.join(root, filename))\n",
    "        return image_path_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can initialize the dataset by providing the directory of the image corpus data\n",
    "image_dir = \"../data/image_corpus/\"\n",
    "dataset = ImageDataset(image_dir=image_dir)\n",
    "\n",
    "# You can use the '_find_files' method to get the list of images paths\n",
    "image_path_list = dataset._find_files(image_dir)\n",
    "\n",
    "# Equivalently, since this list is stored as an inner attribute, you can access it directly:\n",
    "image_path_list = dataset.image_path_list\n",
    "\n",
    "# Display all file paths\n",
    "for image_file_path in image_path_list:\n",
    "    print(image_file_path)\n",
    "print(\"Number of images in the list\", len(image_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the length of the dataset, you can use 'len(dataset)', which calls the method '__len__'\n",
    "print(\"Dataset length:\", dataset.__len__())\n",
    "print(\"Dataset length:\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: load the image with index '0' from the dataset, print its shape, and plot it with the pokemon name\n",
    "# hint: you can use 'dataset[0]', which is the same as '__getitem__(index)'\n",
    "\n",
    "# image, image_name = dataset.__getitem__(0)\n",
    "image, image_name = dataset[0]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "print(image.shape, \"\\nName:\", image_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "A `Dataset` object stores all the data, and it might be very large. In practice, when using deep neural networks, we want to divide it into small packs (or _minibatches_) of data, in order to feed the network and compute stochatstic gradient descent. To that end, we create a `Dataloader`: it's a python class which samples over the dataset (that's the _batch sampler_) and assembles the data and labels (using a _collate function_) to generate batches.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center><a href=\"https://twitter.com/i/status/1363494433715552259\">\n",
    "    <img src=\"https://pbs.twimg.com/ext_tw_video_thumb/1363493414361305099/pu/img/x_qwSxBU2l0o5Y2z.jpg\" width=\"500\"></a>\n",
    "(click on the image above to check the animation)\n",
    "</center>\n",
    "\n",
    "The good news is that there's a Pytorch function that does it automatically, so we don't need to bother with coding the batch sampler nor the collate function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size (=number of samples/images in each batch) and create the dataloader\n",
    "batch_size = 5\n",
    "image_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one batch and print the shape of the image tensor\n",
    "image_batch_data, image_batch_name = next(iter(image_dataloader))\n",
    "print(image_batch_data.shape)\n",
    "\n",
    "# Plot the images along with the corresponding names\n",
    "plt.figure()\n",
    "for ib in range(batch_size):\n",
    "    plt.subplot(1, batch_size, ib + 1)\n",
    "    plt.imshow(image_batch_data[ib, :])\n",
    "    plt.title(image_batch_name[ib])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The total number of batches can be access easily\n",
    "print(\"Dataloader length (number of batches):\", len(image_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: iterate over the dataloader (use a for loop) and print the shape of each batch of image.\n",
    "\n",
    "i = iter(image_dataloader)\n",
    "for j in range(len(image_dataloader)):\n",
    "    batch_data, batch_names = next(i)\n",
    "    print(batch_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should remark from the code above that the last batch has a different size than the others. This is because in the dataset there are 150 images and the `batch_size` is 5: therefore there are 30 batches with 5 images, and the last batch contains the remaining 1 image.\n",
    "\n",
    "To avoid this (which might be useful in some applications where you need to process each batch with the same size), you can get ridd of the last batch with a different size. To do so, simply use the `drop_last=True` option when defining the dataloader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define the dataloader while removing the last batch, and print the length of the new dataloader.\n",
    "batch_size = 5\n",
    "image_dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "print(\"Dataloader length (number of batches):\", len(image_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: iterate over the dataloader and display all images with the name of the pokemon as title\n",
    "for batch_data, batch_names in image_dataloader:\n",
    "    # print(batch_data.shape)\n",
    "    plt.figure()\n",
    "\n",
    "    for ib in range(batch_size):\n",
    "        plt.subplot(1, batch_size, ib + 1)\n",
    "        plt.imshow(batch_data[ib, :])\n",
    "        plt.title(batch_names[ib])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech data\n",
    "\n",
    "Let's now work with speech data. The audio signals are provided in the `data/audio_corpus/` folder.\n",
    "\n",
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an audio file using the sf.read() function\n",
    "x, sampling_frequency = sf.read(\"../data/audio_corpus/arctic_a0001.wav\")\n",
    "\n",
    "# Print the shape of the audio signal and the sampling frequency\n",
    "print(x.shape)\n",
    "print(\"Sampling frequency:\", sampling_frequency, \"Hz\")\n",
    "\n",
    "# The duration of the signal (in seconds) can be computed as follows:\n",
    "duration = len(x) / sampling_frequency\n",
    "print(\"Duration:\", duration, \"s\")\n",
    "\n",
    "# We can use the 'display(Audio())' function to have a graphical audio reader\n",
    "display(Audio(data=x, rate=sampling_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot the audio waveform as a function of time (in seconds)\n",
    "t = torch.arange(len(x)) / sampling_frequency\n",
    "plt.figure()\n",
    "plt.plot(t, x)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dur = 2.0  # maximum duration in seconds\n",
    "max_len = int(\n",
    "    max_dur * sampling_frequency\n",
    ")  # equivalent maximum length in number of samples\n",
    "\n",
    "t = torch.arange(max_len) / sampling_frequency\n",
    "\n",
    "# TO DO: Set the signal at a specified duration 'max_len'\n",
    "# - first, initialize a tensor y from the numpy array x\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "# - then, use an 'if' condition:\n",
    "#     - if the signal length is larger than max_len, then crop it\n",
    "if len(y) > max_len:\n",
    "    y = y[:max_len]\n",
    "\n",
    "#     - if the signal length is smaller than max_len, then add zeros (use cat) accordingly at the end of the signal\n",
    "else:\n",
    "    y = torch.cat((y, torch.zeros(max_len - len(y))))\n",
    "\n",
    "# Plot y\n",
    "plt.figure()\n",
    "plt.plot(t, y)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n",
    "duration = len(y) / sampling_frequency\n",
    "print(\"Duration:\", duration, \"s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and dataloader\n",
    "\n",
    "Now, we can create the speech Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, audio_dir: str, max_dur: float):\n",
    "        \"\"\"Initialize the attributes of the object of the class.\"\"\"\n",
    "        self.audio_dir: str = audio_dir\n",
    "        self.max_dur: float = max_dur\n",
    "        self.audio_path_list: list[str] = sorted(self._find_files(audio_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the dataset.\"\"\"\n",
    "        return len(self.audio_path_list)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"Return a data sample (= speech signal) for a given index.\"\"\"\n",
    "\n",
    "        # TO DO:\n",
    "        # - get the audio path corresponding to 'index' (use the list 'self.audio_path_list')\n",
    "        audio_path = self.audio_path_list[index]\n",
    "\n",
    "        # - load the speech signal into a numpy array x\n",
    "        x, sampling_frequency = sf.read(audio_path)\n",
    "\n",
    "        # - transform x into a pytorch tensor y (you can use torch.from_numpy())\n",
    "        y = torch.from_numpy(x)\n",
    "\n",
    "        # - use a method (which you have to code) to set the length of x at a given duration 'self.max_dur'\n",
    "        y = self._set_duration(y, self.max_dur, sampling_frequency)\n",
    "\n",
    "        # - return the tensor y (which has duration max_dur)\n",
    "        # - also get and return the name of the audio (as for images, use '.replace()')\n",
    "        name = audio_path.replace(self.audio_dir, \"\").replace(\".wav\", \"\")\n",
    "\n",
    "        return y, sampling_frequency, name\n",
    "\n",
    "    def _find_files(self, directory: str, pattern: str = \"*.wav\"):\n",
    "        \"\"\"Recursively find all files matching the pattern.\"\"\"\n",
    "        audio_path_list = []\n",
    "        for root, dirnames, filenames in os.walk(directory):\n",
    "            for filename in fnmatch.filter(filenames, pattern):\n",
    "                audio_path_list.append(os.path.join(root, filename))\n",
    "        return audio_path_list\n",
    "\n",
    "    def _set_duration(\n",
    "        self, y: torch.Tensor, max_dur: float, sampling_frequency: int = 16000\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Set the input signal at a specified duration.\"\"\"\n",
    "        # TO DO: code it\n",
    "        max_len = int(\n",
    "            max_dur * sampling_frequency\n",
    "        )  # equivalent maximum length in number of samples\n",
    "\n",
    "        if len(y) > max_len:\n",
    "            y = y[:max_len]\n",
    "\n",
    "        else:\n",
    "            y = torch.cat((y, torch.zeros(max_len - len(y))))\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Instanciate the dataset (specify the audio directory and a maximum duration of 2 seconds),\n",
    "# Print the dataset's length, and the list of files\n",
    "audio_dir = \"../data/audio_corpus/\"\n",
    "max_dur = 2.0\n",
    "dataset = SpeechDataset(audio_dir, max_dur)\n",
    "\n",
    "print(\"Dataset length:\", len(dataset))\n",
    "\n",
    "audio_path_list = dataset.audio_path_list\n",
    "for audio_file_path in audio_path_list:\n",
    "    print(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: load the speech signal with index '0' from the dataset (and its name), print its shape,\n",
    "# plot the signal (with the name as title) and the graphical audio reader\n",
    "\n",
    "speech_example, speech_example_sampling_frequency, speech_example_name = dataset[0]\n",
    "\n",
    "print(\"Speech signal shape:\", speech_example.shape)\n",
    "\n",
    "t = torch.arange(len(speech_example)) / speech_example_sampling_frequency\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t, speech_example)\n",
    "plt.title(speech_example_name)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.show()\n",
    "\n",
    "display(Audio(data=speech_example, rate=speech_example_sampling_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TO DO: create a dataloader (batch size of 3), iterate over the dataloader and plot all the signals with name as title\n",
    "batch_size = 3\n",
    "speech_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for (\n",
    "    batch_data,\n",
    "    batch_sampling_frequencies,\n",
    "    batch_names,\n",
    ") in speech_dataloader:\n",
    "    # print(batch_data.shape)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    for ib in range(batch_size):\n",
    "        plt.subplot(1, batch_size, ib + 1)\n",
    "        t = torch.arange(len(batch_data[ib])) / batch_sampling_frequencies[ib]\n",
    "        plt.plot(t, batch_data[ib])\n",
    "        plt.title(batch_names[ib])\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "\n",
    "        # display(Audio(data=speech_example, rate=speech_example_sampling_frequency))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text dataset\n",
    "\n",
    "Finally, let's work a bit with text data. Each text file in the corpus consists of a written feedback about a movie, along with a label (1 = positive, 0 = negative).\n",
    "\n",
    "Note that this part is concise because preprocessing textual data is quite involved. It will be studied in more details in the second part of the course (NLP students only).\n",
    "\n",
    "Therefore, the TextDataset class is provided below. Note that in addition to the classical methods (`_init_`, `__getitem__`, `_len__`, and `_find_files`), there are some more methods to preprocess it. In particular:\n",
    "\n",
    "- `_create_vocabulary` gets the list of all words in the corpus to create a vocabulary.\n",
    "- `_tokenize_text` transforms each word into an integer (called _token_).\n",
    "\n",
    "This allows to process textual data as numbers, which is more convenient for neural nets. Checking these methods in details is left as optional work for NLP students.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_dir, max_seq_length=128):\n",
    "        \"\"\"Initialize the attributes of the object of the class.\"\"\"\n",
    "        self.text_dir = text_dir\n",
    "        self.max_seq_len = max_seq_length\n",
    "        self.text_files = sorted(self._find_files(text_dir))\n",
    "        self.labels = self._get_labels()\n",
    "        self.vocab = sorted(self._create_vocabulary())\n",
    "        self.word_to_index = {word: idx for idx, word in enumerate(sorted(self.vocab))}\n",
    "        self.word_to_index[\"[PAD]\"] = -1\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the dataset.\"\"\"\n",
    "        return len(self.text_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a data sample (= text tokens and label) for a given index.\"\"\"\n",
    "        # get the index-th text file from the list of text files defined in __init__\n",
    "        text_file = self.text_files[index]\n",
    "        # return a list of all tokens in the text\n",
    "        tokens = self._tokenize_text(text_file)\n",
    "        # use the word_to_index mapping to transform the tokens into indices and save them into an IntTensor\n",
    "        x = torch.IntTensor([self.word_to_index[word] for word in tokens])\n",
    "        # get the index-th label and store it into a FloatTensor\n",
    "        y = torch.FloatTensor([self.labels[index]])\n",
    "        # stores the text indices and the label into a dictionary\n",
    "        features = {\"token_ids\": x, \"labels\": y}\n",
    "        return features\n",
    "\n",
    "    def _find_files(self, directory, pattern=\"*.txt\"):\n",
    "        \"\"\"Recursively find all files matching the pattern.\"\"\"\n",
    "        files = []\n",
    "        for root, dirnames, filenames in os.walk(directory):\n",
    "            for filename in fnmatch.filter(filenames, pattern):\n",
    "                files.append(os.path.join(root, filename))\n",
    "        return files\n",
    "\n",
    "    def _get_labels(self):\n",
    "        \"\"\"Extract the labels from the given text files.\"\"\"\n",
    "        labels = []\n",
    "        for filepath in self.text_files:\n",
    "            text, label = list(open(filepath, \"r\"))[0].split(\"\\t\")\n",
    "            labels.append(int(label))\n",
    "        return labels\n",
    "\n",
    "    def _create_vocabulary(self):\n",
    "        \"\"\"Create a vocabulary of unique words from the given text files.\"\"\"\n",
    "        all_texts = [\n",
    "            list(open(filepath, \"r\"))[0].strip().lower() for filepath in self.text_files\n",
    "        ]\n",
    "        letters = string.ascii_lowercase\n",
    "        word_string = \" \".join(all_texts)\n",
    "        not_letters = set(\n",
    "            [char for char in word_string if char not in letters and char != \" \"]\n",
    "        )\n",
    "        for char in not_letters:\n",
    "            word_string = word_string.replace(char, \" \")\n",
    "        vocab = set(word_string.split())\n",
    "        return list(vocab)\n",
    "\n",
    "    def _tokenize_text(self, text_file):\n",
    "        \"\"\"\n",
    "        Remove non-characters from the text and pads the text to max_seq_len.\n",
    "        *!* Padding is necessary for ensuring that all text_files have the same size\n",
    "        *!* This is required since DataLoader cannot handle tensors of variable length\n",
    "\n",
    "        Return a list of all tokens in the text\n",
    "        \"\"\"\n",
    "        text = list(open(text_file, \"r\"))[0].strip().lower()\n",
    "        letters = string.ascii_lowercase\n",
    "        not_letters = set(\n",
    "            [char_ for char_ in text if char_ not in letters and char_ != \" \"]\n",
    "        )\n",
    "        for char in not_letters:\n",
    "            text = text.replace(char, \" \")\n",
    "        tokens = text.split()\n",
    "        for i in range(self.max_seq_len - len(tokens)):\n",
    "            tokens.append(\"[PAD]\")\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: create a text dataset (define the text_dir) and display its size.\n",
    "text_dir = \"../data/text_corpus/\"\n",
    "dataset = TextDataset(text_dir=text_dir)\n",
    "print(\"Dataset length:\", len(dataset))\n",
    "\n",
    "# Load the sample of index 0 and print the corresponding token and label.\n",
    "text_example = dataset[0]\n",
    "print(\"Token IDs:\", text_example[\"token_ids\"])\n",
    "print(\"Label:\", text_example[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: create a dataloader for the text data (batch_size=2) and print the contents of each batch (tokens and labels)\n",
    "batch_size = 2\n",
    "text_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for batch_data in text_dataloader:\n",
    "    for ib in range(batch_size):\n",
    "        print(\"Token IDs:\", batch_data[\"token_ids\"][ib])\n",
    "        print(\"Label:\", batch_data[\"labels\"][ib])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
